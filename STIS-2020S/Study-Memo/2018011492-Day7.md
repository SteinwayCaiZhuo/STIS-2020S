# Week 7

**姓名：阿璐思  学号：2018011492**

***

**上节内容的回顾和进一步的深入学习：**

* Tensor形象化表示
  * rank, length, shape, volume
* 单个人工神经元的定义
* 深度神经网络：前馈网络、反馈网络、记忆网络
* Keras
* Neuron -> layer
  * 利用Tensor表示的单个人工神经元来说：输入和参数都是一维的时候，输出是零维的（标量）
  * 输出的shape与layer的shape一致
  * batch的size只会影响输出的shape，不影响参数的shape
* Dense layer的概念和应用

* Softmax层：将输出经过此层处理，计算得到一个概率分布向量
* 利用MLP（由多层Dense组成）可以实现图像分类
* 卷积运算的简介
  * 卷积核（由学习算法得到的权重参数）
  * 卷积网络CNN：局部区域的权重共用空间维度
  * 通常每个卷积层后都紧跟一个下采样层，如最大池化方法进行下采样
  * CNN layer：卷积层、采样层、正则层
  * 卷积层：用于处理图像
  * Pooling层（采样层）：用于采样，缩小模型的大小，但不会改变tensor的深度
  * Dropout层（正则层）：减少CNN过拟合的问题

**Tensorflow机器学习库：**

* 深度学习框架：
  * 描述多层网络模型和训练推断的编程语言以及相关的工具类库
  * 包括编程语言，解释器和编译器
  * 深度学习编译器Tensorflow XLA的简介（加速线性代数）
  * Pytorch编辑器介绍
* Tensorflow的历史发展和简介
  * Tensor是Tensorflow的核心
  * Tensor有constant，placeholder，variables等形式，有rank，shape，datatype等属性
  * Tensorflow示例
  * Tensorflow的求导示例，底层实现
  * Tensorflow的数据流图中包括训练数据的读取转换，队列，参数的更新和周期性监测点的生成，且数据流图中的操作都是并发执行的
  * Tensorflow架构的介绍

**Tensorflow代码示例：**

* 最后由助教对有关代码进行了讲解和运行示范

***

**论文阅读总结：**

​		在该论文中介绍了用于分类大量图像的深度卷积神经网络，该网络由五个卷积层组成，其中部分层后面紧跟max-pooling层进行下采样，除此之外还有三个全连接层，以及为了减少过度拟合的影响利用了dropout层作为正则层，实验结果表明上述结构对于解决图像分类问题效果良好，而这些结构恰好是在本周课上介绍和学习过的。

​		从论文中了解到，在该模型中使用ReLu函数处理非线性段会比使用其他函数进行激活来的更快且错误率更低。此外由于用来训练网络的数据集过大以及单个GPU的内存有限，因此使用了两个GPU配合训练网络，而且这两个GPU只会在certain层上进行交互。接着又定义了一个response normalization，用于在certain层中配合ReLu函数使用。而纵观整个训练模型，它由五个卷积层和三个全连接层组成，通过卷积层不断将输入的image的size减小，最后根据学习结果预测出在1000个分类中最有可能归属的那一类。然后介绍了在实验中试用的两种减少过度拟合的方法Data Augmentation和Dropout，叙述了两者在实际实验中的效果和局限性。最后是一些实际的实验结果和相关效果的示例。

​		总的来说，从初步接触深度学习领域的基本概念和内容到阅读论文了解深度学习在解决实际问题中的应用，我逐步地体会到将实际问题建模到理论框架当中，再利用深度学习中的相关知识进行分解，解决的精妙之处，也感受到其实相关领域的实际应用中看似很复杂的问题都可以通过一定的方法手段分解成简单的问题，再利用学习过的理论知识的错综复杂而又有着缜密的逻辑联系的体系结构去解决实际问题。因此，了解和学习深度学习的相关知识和实际生活中的应用对我们未来的研究和发展方向也是可以起到一定的指引作用，也可以在日常学习中受到一些启发，总之收获不少。