# 学习日志

**姓名：阿璐思 学号：2018011492**

***

**人工神经元：**

* 单个人工神经元：输入的线性加权叠加经过非线性变换（激活函数）输出

* 激活函数：

  * Sigmoid函数：S形

  * tanh函数：双曲正切
  * ReLU函数：整流线性单元

* ReLU单元的具体建模：

  * F(x) = max(0,x)

  * 小于0的部分整流为0，其他部分不变

**人工神经元模拟布尔电路：**

* 逻辑斯特回归单元：Sigmoid函数为激活函数

  * 与或非运算示例

  * 异或运算的两种实现

* 与非门组合可得其他逻辑门
* 利用人工神经元设计判断性别功能：利用经验调参法找到合适的权重

**人工神经网络结构：**

* 按照拓扑连接结构将大量神经元连接起来

  * 前馈网络、反馈网络、记忆网络

* 多层前馈网络

  * 示例：输入层、隐藏层以及输出层三层组成

* 卷积网络（CNN）

  * 局部区域共用权重W
  * 通常每个卷积层后紧跟一个下采样层

* 循环网络（RNN）

  * 权重共享
  * 在不同时间步上采用相同的U、V、W权重矩阵
  * U矩阵：输入到隐藏的连接的参数化的权重矩阵
  * V矩阵：隐藏到输出的连接的参数化的权重矩阵
  * W矩阵：隐藏到隐藏的循环连接的参数化的权重矩阵

  * 可用于时间序列的预测

**人工神经网络的能力：**

* 图像识别
  * 彩色图像与二值图像的简介
  * 例子：输入图像，输出概率向量
* 深度学习的有关数学概念
  * 映射、函数
  * 函数逼近
  * 泛函分析
* 前馈网络万能近似器
  * 万能近似定理：给定一个函数，存在一个前馈网络可以近似干函数
  * Borel可测函数：前馈网络的导数也可以近似函数的导数
* RNN的近似能力
  * 用Sigmoid激活函数的RNN有图灵完备性
  * 任何图灵可计算的函数都可以通过一个有限维的循环网络计算
* 理论与实际存在的问题
  * 理论上存在足够大的网络表示任意函数并且可以达到我们所希望的任意精度
  * 但实际中训练算法不能保证一定能够学得此函数
  * 一是因为用于训练的优化算法有可能找不着用于期望函数的参数值
  * 二是因为训练算法可能由于过拟合而选择错误的函数
* Numpy库的内容介绍

***

**神经网络生物基础：**

* 激活函数的生理基础
  * 神经元结构、动作电位原理简介
  * Hodgkin-Huxley模型
  * Connor-Stevens模型
* 深度学习的生理基础
  * 人的视觉信号处理举例

***

兴趣作业：三人投票表决电路

import torch

import torch.nn as nn

class MAJ(nn.Module):

  def __init__(self):

​    super().__init__()

​    self.ninput = 3

​    self.weight = torch.Tensor([[20, 20, 20]])

​    self.bias = torch.Tensor([-30])



  def forward(self,x):

​    x = F.linear(x, self.weight, self.bias)

​    x = torch.sigmoid(x)

​    return x

***

**总结：**

​		通过本周的学习我对人工神经网络的基本概念有了一定的了解和认识，同时学习了Numpy库的相关函数的操作和用法，学习了如何利用人工神经网络实现布尔电路，以及如何利用Pytorch对设计的人工神经网络进行验算，对人工神经网络的实现有了基本认识。