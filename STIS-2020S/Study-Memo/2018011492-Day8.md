# Week 8

**姓名：阿璐思  学号：2018011492**

***

**RNN：**

* 网络结构介绍：训练目标、损失函数、网络输入、网络输出、状态

* 权重共享：在不同的时间不上采用相同的权重矩阵（U、V、W）
  * U矩阵：输入层到隐藏层的连接的参数化的权重矩阵
  * V矩阵：隐藏层到输出层的连接的参数化的权重矩阵
  * W矩阵：隐藏层到隐藏层的连接的参数化的权重矩阵
* 计算图、Basic-RNN的举例解释
* RNN-Cell的抽象：
  * Input + State -> Output + NewState
* 对RNN网络的输入要序列化，时间步上权重共享，再根据输出与理论值进行对比得到损失函数

**LSTM：**

* 通过LSTM可以解决RNN训练存在的问题：RNN梯度爆炸、RNN梯度消失
* 门单元Gate：Input、Control的Tensor以及Output的形状相同
* LSTM通过增加1个辅助记忆单元和3个辅助的门单元Gate，对Vanilla RNN进行了改进
  * 三个Gate分别为输入门（控制输入）、遗忘门（控制存储）、输出门（控制输出）
  * 辅助记忆单元寄存时间序列的输入，训练过程中利用后向传播的方式进行
  * 解决了RNN网络收敛慢的问题
* LSTM的举例解释
* GRU layer：
  * LSTM的遗忘门和输入门合并成一个更新门
  * 利用重置门选择是否忽略历史状态信息
  * Cell状态和隐藏态合并成一个状态

**自动微分：**

* 微分方式：Manual differentiation（人工）、Numerical differentiation（数学方法）、Symbolic differentiation（符号微分）、Automatic differentiation（自动微分）
* 前馈模式：利用雅可比矩阵求解
* 反向传播调整网络权重参数
* 利用图的概念（有向无环图）进行自动微分操作

**ASR的基本介绍：**

* 自动语音识别的概念、背景以及所存问题
* 经典架构、评价指标WER和CTC折叠的介绍

***

**课后阅读作业：** 

​        本次阅读作业是有关基于深度循环神经网络的语音识别的论文，在该论文中让我印象比较深的是采用BRNN在某一语句的上下文的两个方向上采用两个独立隐藏层实现更好的语音识别效果，而不是只根据单一的语句进行识别，这样能够更好地保证语音输入人想要表达的意思的完好的转。还有一个就是关于利用深度RNN和LSTM，把多个RNN的隐藏层叠加起来构成深度RNN网络，再在隐藏层中使用LSTM构造深度出LSTM，实验结果表明语音识别效果要比单一的LSTM的识别要更好。在具体实现部分，有CTC网络以及预测网络，提到了利用具体的训练数据预先训练预测网络，与人类学习过程中的预习比较相似，通过实验结果可以发现该论文中提出的上下文两个方向的end-to-end的训练在实际问题的处理中还是有比较可观的效果的。除此之外，此次阅读更多的印象是感受到将实际问题，尤其是像语音识别这种处理起来比较困难繁琐的问题转化成理论层面的深度学习的问题还是比较麻烦和困难的，而且个人感觉通过深度学习实现语音识别要比解决图像分类之类的问题要更难，网络结构可能更复杂些，总之感觉未来语音识别这一方向还是有很多可以提升的空间的。

