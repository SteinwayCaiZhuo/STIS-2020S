# 学习日志

**姓名：阿璐思  学号：2018011492**

***

**人工神经网络人物：**

* 深度学习界的领军人物：

  * Geoffrey Hinton、Yoshua Bengio、 Yann Lecun

  * 反向传播算法BP、Autooencoder和机器翻译的GRU、用卷积网络识别手写体的方法等

  * 标志性的论文：深度信仰网络、深度卷积网络、深度循环网络

* 人工神经网络训练：

  * DIY人工神经元以及多层全连接网络
  * 利用确定目标函数、反向传播与自动微分、修正权重等步骤训练网络
  * 利用带标签的训练样本对神经网络进行训练，进而确定网络的权重参数
  * 对比神经网络实际输出与理论预期输出得到差异
  * 将差异进行最小化的操作，此处用度量函数衡量，如绝对值求和、平方和、交叉熵等，称为损失函数
  * 交叉熵通常用于分类任务时作损失函数
  * 利用梯度下降法进行反向传播，进而调整神经网络的权重参数
  * 随机梯度下降方法是最常用的权重调节方法
    * 随机初始化每个神经元输入权重和偏差
    * 选取一个随机样本
    * 根据网络的输出结果，从最后一层开始，逐层计算每一层权重的偏导数
    * 逐层调整每层的权重，更新权重
    * 再次随机选取样本，开始下一轮迭代
    * 核心是**选取随机样本**
  * 深度网络的实际训练过程中采用分批训练的方式可以保证训练的收敛性
    * 利用共轭梯度法和L-BFGS加速技术在此方式中效果明显
    * 缺点是模型参数的更新需要遍历整个的数据集
  * 除此之外通常使用小批量训练的方式训练网络，是批量训练和随机梯度下降法的折中

* 人工神经网络实现

  * 数值分析的基本概念
  * 数值计算的误差来源有：模型误差、观测误差、方法误差和舍入误差等
  * 数值计算的四项基本原则
    * 防止大数吃小数
    * 避免相近数相减
    * 避免除数绝对值远小于被除数绝对值的除法
    * 简化计算步骤，减少运算次数
  * 求导运算以及有关微分的概念

* Tensorflow的实现

  * 数据Tensor与张量Tensor的概念
  * 利用张量表示多层全连接网络

* Tensorflow的基本预备知识

  * Session、Graph、Eager Execution
  * Tensor的三种表现形式：variable、placeholder、constant
  * 可以利用constant实现tensor的简单计算，可以利用matrix对其进行操作
  * 人工神经元Neuron模型的介绍：梯度下降、Loss损失函数等
  * 一些关于tensorflow的简单例子的动态演示

***

**总结：**

​		通过本周的课程进一步学习了深度学习的有关知识与Tensorflow的基本概念，对深度学习网络的训练过程和方法以及细节的实现有了更深刻的认识，同时也通过tensorflow例子的动态演示对深度学习网络的实用性和直观性有了更多的体会。总而言之，本周课程是对深度学习网络的一次很好的带动引入。