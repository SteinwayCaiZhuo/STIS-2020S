沈梓超 2018011043 第八次课程

Tensorflow扩展介绍

#### RNN（循环神经网络）

计算图：

U、V和W分别对应于输入到隐藏、隐藏到隐藏、输出到隐藏
rnn-cell抽象
输入和loss处理
对输入进行one-hot处理（离散化处理）

#### LSTM（长短时记忆网络）

RNN训练存在梯度爆炸和梯度消失问题，LSTM解决了这两个问题
门单元Gate：Control经过sigmoid函数后，变成一个范围在0-1之间的
增加三个辅助的门单元和一个记忆单元，可以寄存时间序列的输入，在训练过程中会利用后向传播的方式

#### GRU

只有两个门，将LSTM的遗忘门和输入门阻和为单一的更新门，同时加入重置门，将隐藏态和cell态合并为一个状态

#### 自动微分

##### background

1. 手动微分
	手动计算，再将其手动输入到代码中
	局限性，当神经网络过于复杂时，不再适用
2. 通过定义进行数值计算：在h趋近于0时，会存在精度不足的问题，同时计算量较大
3. 利用求导法则进行求导运算
	存在表达式爆炸的问题，计算量相对也比较大
4. 把链式法则运用到计算图上
	任何一个计算图必须是有向无环图

##### forward mode

##### 对偶数

##### 反向传播

在实际应用中，输入的数量是远远大于输出的数量的，所以反向模式是更为适用的

课后阅读作业2：

论文研究了在深度递归神经网络中，将有效的多层次表示和RNN的远程上下文相结合，来优化了在语音识别中端到端的训练。

论文中证明了深度双向长短期内存RNN与端到端训练和权重噪声的结合为TIMIT数据库中的音素识别提供了最先进的结果，对后期的研究工作：将系统扩展到大词汇量语音识别有很大帮助。