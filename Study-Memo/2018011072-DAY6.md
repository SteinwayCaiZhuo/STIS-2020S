# DAY6

#### 王景维 无82班 2018011072

### 这周学了深度网络的一些基本的知识

* 损失函数，一般用平方和，交叉熵。对于回归任务，用均方误差；对于分类任务，用交叉熵
* 用梯度下降法来让学习，让损失函数最小化。梯度下降法即沿着函数的负梯度方向迭代
* 在实际中，一般使用小批量训练，这是批量训练和随机梯度下降法的折中。

### 学了一些tensorflow的简单使用

* dy_dx=gradient(y,x)可以用来求梯度，但y和x要有关系，一般按下面的用法来

  ```
  with tf.GradientTape() as t:
  	t.watch(x)
  	y=f(x)
  dy_dx=gradient(y,x)
  ```

  

