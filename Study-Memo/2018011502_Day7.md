# Week_7

###### 姓名：李卓峰  班级：自83  学号：2018011502

#### 一、上周遗留的问题

上周在**pip install tensorflow**后，安装的包里是有tensorflow，但在引入时却会报错：**ImportError: DLL load failed: 找不到指定的模块**。当时找了好多原因，比如安装**Microsoft visual C++2015**等等，但都没有用，后来在一篇CSDN中找到了解决方案，是因为**pip install tensorflow**安装的是最新的版本，而最新的版本和我的python版本不匹配，重新安装另一个版本的tensorflow后就可以正常的import tensorflow了。Blog链接：https://blog.csdn.net/weixin_43325818/article/details/86480384。

#### 二、TensorFlow机器学习库

- 深度学习框架是描述多层网络模型及训练推断的编程语言及工具类库，包括编程语言，解释器和编译器。
- **XLA**（加速线性代数）是一种能够优化TensorFlow计算的编译器
- **Pytorch**的架构
- **TensorFlow**会自动对代码求导，优化权重参数，使得损失函数最小。求导采用符号微分方法。数据流图：TensorFlow用于模型训练过程的数据流图，包括训练数据的读取和转换，队列，参数的更新以及周期性监测点生成；图中的操作都是并发执行的，图中的节点的可变状态在在图的执行中是可以共享的。TensorFlow架构：上层是训练层和推断库，中间层是Python和C++接口，底层是网络层和设备层。

#### 三、Dense&CNN&RNN

- 深度神经网络（Deep Networks）：按照拓扑连接结构，将大量的神经元组织起来，构成规模化的深度的网络；每层不同神经元之间的连接关系，即层间的连接关系；前馈网络、反馈网络、记忆网络。
- **Keras**是一套高级API，用来快速搭建深度神经网络。
- **Dense**：单个人工神经元输入和参数都是1d(1维)，输出是标量即0d。一层人工神经元构成一个Layer，输出的shape和Layer的shape一致；batch_size——会影响输出的shape与不影响参数的shape。
- **Dense layer**：Layer的结构是一维的。超参数指的是神经元的个数U。
- **Softmax层**：计算出一个概率分布的向量；所有输出的数值是正的，所有分量之和为1.
- **CNN**（卷积网络，Convolutional neural network）：卷积运算；例子：2d卷积核。特点：局部区域权重W共用；每一个卷积层后通常跟着一个下采样层；例子：3d卷积核
- **CNN layer**包括卷积层（*convolutional layers*）、采样层（*pooling layers*）和正则层（*normalization layers*）（如dropout层）。
- 卷积层用于处理图像，Layer的结构是3d。超参数包括卷积核个数（D）、核大小（F）、padding(P)以及strides（S）
- 采样层用于采样、缩小模型大小。Layer的结构是3d，超参数包括pooling_type、window_shape、padding、strides。但采样层不会改变tensor的深度。
- Dropout层：减少CNN过拟合问题，超参数为keep_prob(丢弃率)。对于所有的输入，有keep_prob概率保留并乘以1/keep_prob，以保证前后总和大致相等，否则输出0.

#### 四、论文

- 该神经网络架构：八个学习层(*learned layers*)：五个卷积层(*convolutional layers*)和三个全连接层(*fully-connected layers*)。
- 第一个特点：用ReLU训练的深度卷积神经网络比用tanh训练的快好几倍。
- 第二个特点：用两个GPU并行处理数据，训练所用的时间比一个GPU短。
- 第三个特点：局部响应归一化(*Local Response Normalization*)提高了一定的精度。但这个原理没太看懂。
- 第四个特点：重叠池化(Overlapping Pooling, 但课件中Pooling layers翻译成了采样层):就是采样的时候，将采样的区域大于采样间隔。
- 第五个特点：介绍了一下全部的架构，每层的输入输出等。
- 两个减少过拟合的方法：
  1. 数据集扩增——通过使用保留标签转换(*using label-preserving transformations*)来人为地扩大数据集，包括两种方式，其一是图像转化和水平反射(*generating image translations and horizontal reflections*)，其二是改变训练图片中RGB通道的强度。
  2. Dropout—— 在深度学习网络的训练过程中，按照一定的概率将一部分神经网络单元暂时从网络中丢弃。 
- 训练细节：介绍了训练模型的方法——随机梯度下降法，说明了权重的更新公式，然后介绍了模型的初始值等。
- 结果：错误率得到了明显的降低。
- 不懂的地方：overfit,saturating

#### 五、学习中遇到的困难与问题

1. 对一些概念还不知道或者不够熟悉，比如overfitting和saturating。
2. 虽然学习到了一些layer，但不知道他们怎么用（或者说不知道他们的应用），比如*softmax*层。
3. 不知道如何实现深度学习的各个层，对Tensorflow和Keras等不会使用，从给的示例代码上看，tensorflow更像是一种全新的语言。没有经过系统的学习，不会使用这个库。
4. 对深度学习的数学原理掌握不足，即使知道了数学原理，也不知道他为什么能用到深度学习。比如卷积网络，目前只是明白了卷积层进行的操作，但不知道这种操作有什么用，这方面的问题跟问题2有点相似。

总之，学习深度学习任重道远，需要不断努力加油。